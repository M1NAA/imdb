{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxdWVttMaaHtB6LizAT1Ul",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M1NAA/imdb/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "id": "P9EnnWmfB7Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar /content/News_Articles.rar /content/News_Articles"
      ],
      "metadata": {
        "id": "AGNvpj7TCmQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JnPRUcavA-bi"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "import streamlit as st\n",
        "import nltk\n",
        "from urllib import request \n",
        "from bs4 import BeautifulSoup as bs\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import heapq\n",
        "#from nltk.stem.isri import ISRIStemmer\n",
        "from nltk.corpus import PlaintextCorpusReader ,stopwords\n",
        "#import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict = {\n",
        "    \n",
        "'English': 'English',\n",
        "'Arabic': 'Arabic',\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "Aw8qe6H7BHqn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st.title(\"summarization with nlp\")"
      ],
      "metadata": {
        "id": "Dn4L3KbSBZ2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with st.form(\"form1\"):\n",
        "    \n",
        "    option = st.selectbox('choose the topic you want to summarize',('English', 'Arabic')) #, 'tech'))\n",
        "    \n",
        "    \n",
        "    path = '/content/News_Articles/' + my_dict[option]\n",
        "    texts = []\n",
        "    for topics in PlaintextCorpusReader(path, '.*').fileids():\n",
        "        file = open(path+\"\\\\\"+topics, \"r\", encoding=\"utf8\")\n",
        "        texts.append(file.read())\n",
        " \n",
        "    topic = st.number_input('Enter the number of text you want to summarize from 1 to 200 :', min_value=0, max_value=153, value=0, step=1)\n",
        "    texts = texts[topic]\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    submited = st.form_submit_button(label='summarize')\n",
        "    if submited:\n",
        "        if texts is not None:\n",
        "            def preprocess_text(text):\n",
        "                # Remove URLs and HTML tags\n",
        "                text = re.sub(r'http\\S+', '', text)\n",
        "                text = re.sub(r'<.*?>', '', text)\n",
        "                text = re.sub(r'ØŒ', '', text)\n",
        "                text = text.replace(\"\\\"\", \"\")\n",
        "\n",
        "                # Tokenize text\n",
        "                tokens = text.lower()\n",
        "                tokens = text.split()\n",
        "\n",
        "                # Remove punctuation\n",
        "                exclude = set(string.punctuation) - set('.')\n",
        "                tokens = [ch for ch in tokens if ch not in exclude]\n",
        "\n",
        "                # Remove stop words\n",
        "                #stop_words = stopwords.words('english')\n",
        "                #filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "                stop_words = stopwords.words('arabic')\n",
        "                filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "                # Lemmatize tokens\n",
        "                lemmatizer = WordNetLemmatizer()\n",
        "                lemmas = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "                # Join lemmas back into a single string\n",
        "                preprocessed_text = ' '.join(lemmas)\n",
        "\n",
        "                sentences_tokens = nltk.sent_tokenize(preprocessed_text)\n",
        "                \n",
        "                return lemmas,sentences_tokens\n",
        "    \n",
        "            words_tokens,sentences_tokens = preprocess_text(texts)\n",
        "\n",
        "            word_frequencies = {}\n",
        "            for word in words_tokens:\n",
        "                if word not in word_frequencies.keys():\n",
        "                    word_frequencies[word] = 1\n",
        "                else:\n",
        "                    word_frequencies[word] += 1\n",
        "                    \n",
        "                    \n",
        "            number_of_tokens = len(word_frequencies)\n",
        "            for word in word_frequencies.keys():\n",
        "                word_frequencies[word] = (word_frequencies[word]/number_of_tokens)    \n",
        "\n",
        "            sentences_scores = {}\n",
        "            wordsCounter = 0.0\n",
        "            for sentence in sentences_tokens:\n",
        "                for word in nltk.word_tokenize(sentence):\n",
        "                    if word in word_frequencies.keys():\n",
        "                        wordsCounter += 1;\n",
        "                        \n",
        "                        if sentence not in sentences_scores.keys():\n",
        "                            sentences_scores[sentence] = word_frequencies[word]\n",
        "                        else:\n",
        "                            sentences_scores[sentence] += word_frequencies[word]\n",
        "                sentences_scores[sentence] = sentences_scores[sentence]/wordsCounter\n",
        "                wordsCounter = 0\n",
        "\n",
        "\n",
        "            Summary = heapq.nlargest(int(len(sentences_tokens)/2), sentences_scores, key=sentences_scores.get)\n",
        "            final_summray = []\n",
        "            if len(Summary) <= 1:\n",
        "                final_summray.append(' '.join(Summary))\n",
        "            else:\n",
        "                for text in sentences_scores.keys():\n",
        "                    if text in Summary:\n",
        "                        final_summray.append(text)\n",
        "                        \n",
        "                final_summray = ' \\n'.join(final_summray)        \n",
        "\n",
        "\n",
        "\n",
        "            st.success(\"the original text:-  \")\n",
        "            st.text(texts)\n",
        "            \n",
        "            \n",
        "            st.success(\"summarized text:- \")\n",
        "            st.text(final_summray)"
      ],
      "metadata": {
        "id": "6GB_GnubCGkK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}